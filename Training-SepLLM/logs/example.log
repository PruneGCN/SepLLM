[2025-01-14 11:07:24,226] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
Warning: The default cache directory for DeepSpeed Triton autotune, /home/txiao/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
NeoXArgs.from_ymls() ['../../sample_configs/sepllm-160m-on-pythia-with-pile_deduped-n64HT-kernel_recompile_rotaryBiPE.yml']
INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 2
-------------------- arguments --------------------
  attention_config ................ ['global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global']updated
  batch_size ...................... 64..........................updated
  bias_gelu_fusion ................ True........................updated
  BiPE_seps ....................... [15, 13, 32, 2, 28, 27, 209, 186, 187]updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 1000........................updated
  config_files .................... {'sepllm-160m-on-pythia-with-pile_deduped-n64HT-kernel_recompile_rotaryBiPE.yml': '{\n  # ##########Debug##########\n  #   "world_size": 1,\n  #   "num_gpus": 1,\n  # ########################\n\n  # parallelism settings\n  "pipe-parallel-size": 1,\n  "model-parallel-size": 1,\n\n  # model settings\n  "num-layers": 12,\n  "hidden-size": 768,\n  "num-attention-heads": 12,\n  "seq-length": 2048,\n  "max-position-embeddings": 2048,\n  "pos-emb": "rotary",\n  "rotary-pct": 0.25,\n  "no-weight-tying": true,\n  "gpt-j-residual": true,\n  "output-layer-parallelism": "column",\n  "attention-config": [[["global"], 12]], \n\n  "scaled_masked_softmax_fusion": true, # For SepLLM\n  "bias-gelu-fusion": true,\n\n  # init methods\n  "init_method": "small_init",\n  "output_layer_init_method": "wang_init",\n\n  "optimizer": {\n    "type": "Adam",\n    "params": {\n      "lr": 0.0006,\n      "betas": [0.9, 0.95],\n      "eps": 1.0e-8\n    }\n  },\n  "min_lr": 0.00006,\n\n  "zero_optimization": {\n    "stage": 1,\n    "allgather_partitions": true,\n    "allgather_bucket_size": 500000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 500000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n  # batch size (trained on 8 gpus)\n  "train_micro_batch_size_per_gpu": 64, # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards\' machine, to keep the global train_batch_size as 1024.\n  "gradient_accumulation_steps": 2,  # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards\' machine, to keep the global train_batch_size as 1024.\n  "data-impl": "mmap",\n  "num_workers": 1,\n\n  # activation checkpointing\n  "checkpoint-activations": true,\n  "checkpoint-num-layers": 1,\n  "partition-activations": true,\n  "synchronize-each-layer": true,\n\n  # regularization\n  "gradient_clipping": 1.0,\n  "weight-decay": 0.1,\n  "hidden-dropout": 0,\n  "attention-dropout": 0,\n\n  # precision settings\n  "fp16": {\n    "fp16": true,\n    "enabled": true,\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "initial_scale_power": 12,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n\n  "train-iters": 143000,\n  "lr-decay-iters": 143000,\n  "distributed-backend": "nccl",\n  "lr-decay-style": "cosine",\n  "warmup": 0.01,\n  "checkpoint-factor": 1000,\n  "extra-save-iters": [0,1,2,4,8,16,32,64,128,256,512],\n  "eval-interval": 4000, \n  "eval-iters": 10,\n\n  "log-interval": 10,\n  "steps_per_print": 10,\n  "wall_clock_breakdown": true,\n\n  "train-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],\n  "valid-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],\n  "test-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],\n\n  "tokenizer-type": "HFTokenizer",\n  "vocab-file": "/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json",\n\n  "launcher": "pdsh",\n\n  "save": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE",\n  "load": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE",\n  \n  # "save": "path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE",\n  # "load": "path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE",\n\n  ####################################################################### SepLLM #######################################################################:\n  # "hostfile": "path/to/SepLLM/Training-SepLLM/sample_configs/hostfile",\n\n  \'separator_token_ids\': [15, 13, 32, 2, 28, 27, 209, 186, 187], # For Pythia tokenizer  ## Fixed; The token ids for the special tokens (i.e. separators):  [\'.\', \',\', \'?\', \'!\', \';\', ":", \' \', \'\\t\',\'\\n\'].\n  \'PADDING_ID\': 0 , # For Pythia tokenizer  ## Fixed; The id for padding token of Pythia (GPT_NeoX)\n  \n\n  \'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS\'  :   True,  # If True: the prefilling local window sizes for different self-attention layers are different; If True: should set \'prefill_loc_win_size_list\', else: should set \'prefill_local_window_size\'\n  \'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS\' :   True,  # If True: the generating local window sizes for different self-attention layers are different; If True: should set \'generate_win_loc_size_list\', else: should set \'generate_local_window_size\'. USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS does not have any effect during the pretraining/prefilling phase.    \n\n  \'prefill_loc_win_size_list\' : [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when training (or prefilling). KVs for tokens inside the local window (we call them \'Neighboring Tokens\') are kept and can been seen by the current token.                      \n\n  \'generate_win_loc_size_list\': [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when generating. KVs for tokens inside the local window (we call them \'Neighboring Tokens\') are kept and can been seen by the current token. generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.\n\n  \'init_tok_max_idx\' :  2, # The largest index for the kept initial tokens. E.g., if init_tok_max_idx==2, it means we keep 3 initial tokens (idx: 0,1,2)\n  ######################################There should be at most 1 True for the following 3 args ##############################################\n  \'USE_ORIGINAL_FULL_ATTEN\' : False,  # Flag signal with the highest priority.  Train the Pythia model without any modification (standard full-attention version, i.e., standard upper triangular mask) if True.\n  \'streamingLLM\' :  False,  # Train streamingLLM. Only takes effect when USE_ORIGINAL_FULL_ATTEN=False. \n  \'USE_SEP_ATTN_KERNEL_ACCELERATOR\': True, # If True, use Sep_Attention module\'s kernel accelerator to accelerate the training process of SepLLM. If False (together with USE_ORIGINAL_FULL_ATTEN=False and streamingLLM=False), run plain SepLLM\n  ######################################There should be at most 1 True for the above 3 args ##############################################\n\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n  \'RECOMPILE_SEP_ATTN_KERNEL\': True,  # False by default. If True, recompile the Sep_Attention kernels.  When set to True, it may require more GPU memory and provide a certain level of acceleration to the training process.\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n\n  \'BATCH_ADAPTIVE_INIT_POS\' : False,  # False by default.  If True: use the floating initial tokens\' starting positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding). Can be False when pretraining since the starting positions of initial tokens are at the beginning of each sequence in a batch for pretraining (i.e., right padding)\n  \'PRINT_KV_RATIO\' :  True,  # If True, print the KV cache preservation ratio (especially for the released trained model during generating). When pretraining, it will also print the retention ratio for the computational complexity of calculating the attention map if it is set True\n  \'print_ratio_intervals\': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every \'print_ratio_intervals\' forward passes (or print_ratio_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    \n\n  \'USE_BiPE\':  True,  # False by default. If True (must also set pos_emb=\'rotary\' or \'alibi\'), use Bilevel Positional Encoding [He, Zhenyu, et al. "Two stones hit one bird: Bilevel positional encoding for better length extrapolation." arXiv preprint arXiv:2401.16421 (2024).].\n  \'BiPE_seps\': [ 15, 13, 32, 2, 28, 27, 209, 186, 187 ],  # The token ids of the seperator tokens for BiPE. e.g.,  [\'.\', \',\', \'?\', \'!\', \';\', ":", \' \', \'\\t\',\'\\n\'] \n  ####################################################################### ### #######################################################################\n}\n'}updated
  data_impl ....................... mmap........................updated
  dynamic_loss_scale .............. True........................updated
  eval_interval ................... 4000........................updated
  eval_iters ...................... 10..........................updated
  extra_save_iters ................ [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
  generate_win_loc_size_list ...... [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048]updated
  global_num_gpus ................. 2...........................updated
  gpt_j_residual .................. True........................updated
  gradient_accumulation_steps ..... 2...........................updated
  hidden_size ..................... 768.........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  load ............................ /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPEupdated
  log_interval .................... 10..........................updated
  lr .............................. 0.0006......................updated
  lr_decay_iters .................. 143000......................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  min_lr .......................... 6e-05.......................updated
  no_weight_tying ................. True........................updated
  num_attention_heads ............. 12..........................updated
  num_layers ...................... 12..........................updated
  num_workers ..................... 1...........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  partition_activations ........... True........................updated
  pipe_parallel_size .............. 1...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  prefill_loc_win_size_list ....... [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048]updated
  PRINT_KV_RATIO .................. True........................updated
  RECOMPILE_SEP_ATTN_KERNEL ....... True........................updated
  rotary_pct ...................... 0.25........................updated
  save ............................ /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPEupdated
  save_iters ...................... [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000]updated
  scaled_masked_softmax_fusion .... True........................updated
  separator_token_ids ............. [15, 13, 32, 2, 28, 27, 209, 186, 187]updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  synchronize_each_layer .......... True........................updated
  test_data_paths ................. ['/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document']updated
  test_data_weights ............... [1.0].......................updated
  text_gen_type ................... unconditional...............updated
  tokenizer_type .................. HFTokenizer.................updated
  train_batch_size ................ 256.........................updated
  train_data_paths ................ ['/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document']updated
  train_data_weights .............. [1.0].......................updated
  train_iters ..................... 143000......................updated
  train_micro_batch_size_per_gpu .. 64..........................updated
  USE_BiPE ........................ True........................updated
  USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS  True.................updated
  USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS  True..................updated
  user_script ..................... ../../train.py..............updated
  valid_data_paths ................ ['/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document']updated
  valid_data_weights .............. [1.0].......................updated
  vocab_file ...................... /lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.jsonupdated
  wall_clock_breakdown ............ True........................updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  _DEFAULT_SPARSE_BLOCK_SIZE ...... 128.........................default
  account ......................... None........................default
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  allow_chopped ................... True........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_dropout ............... 0...........................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  BATCH_ADAPTIVE_INIT_POS ......... False.......................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  clip_grad ....................... 1.0.........................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  create_moe_param_group .......... True........................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_path ....................... None........................default
  data_types ...................... None........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  enable_expert_tensor_parallelism  False.......................default
  eod_mask_loss ................... False.......................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  EXCLUDE_DIAGONAL ................ True........................default
  exit_interval ................... None........................default
  expert_interval ................. 2...........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  generate_k ...................... 0...........................default
  generate_local_window_size ...... 256.........................default
  git_hash ........................ None........................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_tied ...................... False.......................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  hidden_dropout .................. 0...........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  init_tok_max_idx ................ 2...........................default
  intermediate_size ............... None........................default
  iteration ....................... None........................default
  keep_last_n_checkpoints ......... None........................default
  launcher ........................ pdsh........................default
  layernorm_epsilon ............... 1e-05.......................default
  layernorm_fusion ................ False.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_dir ......................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  make_vocab_size_divisible_by .... 128.........................default
  mamba_causal_conv_fusion ........ False.......................default
  mamba_inner_func_fusion ......... False.......................default
  mamba_selective_fp32_params ..... True........................default
  mamba_selective_scan_fusion ..... False.......................default
  mamba_use_bias_in_conv .......... True........................default
  mamba_use_bias_in_linears ....... False.......................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  memory_profiling ................ False.......................default
  memory_profiling_path ........... None........................default
  merge_file ...................... None........................default
  min_scale ....................... 1.0.........................default
  mlp_type ........................ regular.....................default
  mmap_warmup ..................... False.......................default
  model_parallel_size ............. 1...........................default
  moe_eval_capacity_factor ........ 1.0.........................default
  moe_expert_parallel_size ........ 1...........................default
  moe_glu ......................... False.......................default
  moe_jitter_eps .................. None........................default
  moe_lbl_in_fp32 ................. False.......................default
  moe_loss_coeff .................. 0.1.........................default
  moe_min_capacity ................ 4...........................default
  moe_num_experts ................. 1...........................default
  moe_token_dropping .............. False.......................default
  moe_top_k ....................... 1...........................default
  moe_train_capacity_factor ....... 1.0.........................default
  moe_type ........................ megablocks..................default
  moe_use_residual ................ True........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  no_ssh_check .................... False.......................default
  norm ............................ layernorm...................default
  num_gpus ........................ None........................default
  num_kv_heads .................... None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_parallelism ........ column......................default
  override_lr_scheduler ........... False.......................default
  pack_impl ....................... packed......................default
  padded_vocab_size ............... None........................default
  PADDING_ID ...................... 0...........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  prefill_k ....................... 0...........................default
  prefill_local_window_size ....... 256.........................default
  prescale_gradients .............. False.......................default
  print_ratio_intervals ........... 8000........................default
  profile ......................... False.......................default
  profile_backward ................ False.......................default
  profile_step_start .............. 10..........................default
  profile_step_stop ............... 12..........................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  return_logits ................... False.......................default
  rms_norm_epsilon ................ 1e-08.......................default
  rope_fusion ..................... False.......................default
  rotary_emb_base ................. 10000.......................default
  rotary_save_freqs_buffer ........ False.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  s3_chunk_size ................... 104857600...................default
  s3_path ......................... None........................default
  SA_Denominator_Bias ............. 1e-10.......................default
  SA_Numerator_Bias ............... 0.0.........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save_base_shapes ................ False.......................default
  scaled_upper_triang_masked_softmax_fusion  False..............default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  sequence_parallel ............... False.......................default
  short_seq_prob .................. 0.1.........................default
  sliding_window_width ............ None........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  steps_per_print ................. 10..........................default
  streamingLLM .................... False.......................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  tensorboard_dir ................. None........................default
  test_label_data_paths ........... None........................default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_label_data_paths .......... None........................default
  use_bias_in_attn_linear ......... True........................default
  use_bias_in_norms ............... True........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_cpu_initialization .......... False.......................default
  use_mup ......................... False.......................default
  USE_ORIGINAL_FULL_ATTEN ......... False.......................default
  use_qk_layernorm ................ False.......................default
  USE_SA_SOFTMAX .................. False.......................default
  USE_SA_SOFTMAX_NO_DENO .......... False.......................default
  USE_SEP_ATTN_KERNEL_ACCELERATOR . True........................default
  use_shared_fs ................... True........................default
  use_tutel ....................... False.......................default
  use_wandb ....................... None........................default
  valid_label_data_paths .......... None........................default
  wandb ........................... None........................default
  wandb_group ..................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weight_decay .................... 0.1.........................default
  weighted_sampler_alpha .......... 1.0.........................default
  world_size ...................... None........................default
---------------- end of arguments ----------------
Warnings: It is recommended to set the value of generate_win_loc_size_list=[2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048] to be the same as prefill_loc_win_size_list=[2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], even though generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.
NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 
[2025-01-14 11:07:35,355] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-01-14 11:07:35,355] [INFO] [runner.py:585:main] cmd = /home/txiao/miniconda3/envs/py39_cu121_torch251_new2/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../../train.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogMjU2LCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogNjQsICJncmFkaWVudF9hY2N1bXVsYXRpb25fc3RlcHMiOiAyLCAib3B0aW1pemVyIjogeyJ0eXBlIjogIkFkYW0iLCAicGFyYW1zIjogeyJsciI6IDAuMDAwNiwgImJldGFzIjogWzAuOSwgMC45NV0sICJlcHMiOiAxZS0wOH19LCAiZnAxNiI6IHsiZnAxNiI6IHRydWUsICJlbmFibGVkIjogdHJ1ZSwgImxvc3Nfc2NhbGUiOiAwLCAibG9zc19zY2FsZV93aW5kb3ciOiAxMDAwLCAiaW5pdGlhbF9zY2FsZV9wb3dlciI6IDEyLCAiaHlzdGVyZXNpcyI6IDIsICJtaW5fbG9zc19zY2FsZSI6IDF9LCAiemVyb19vcHRpbWl6YXRpb24iOiB7InN0YWdlIjogMSwgImFsbGdhdGhlcl9wYXJ0aXRpb25zIjogdHJ1ZSwgImFsbGdhdGhlcl9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgIm92ZXJsYXBfY29tbSI6IHRydWUsICJyZWR1Y2Vfc2NhdHRlciI6IHRydWUsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IHRydWUsICJjcHVfb2ZmbG9hZCI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZX0= --megatron_config {"train_batch_size": 256, "train_micro_batch_size_per_gpu": 64, "gradient_accumulation_steps": 2, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "initial_scale_power": 12, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 12, "hidden_size": 768, "num_attention_heads": 12, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "lr_decay_style": "cosine", "lr_decay_iters": 143000, "min_lr": 6e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0006, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "test_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "valid_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "train_data_weights": [1.0], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "save": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE", "config_files": {"sepllm-160m-on-pythia-with-pile_deduped-n64HT-kernel_recompile_rotaryBiPE.yml": "{\n  # ##########Debug##########\n  #   \"world_size\": 1,\n  #   \"num_gpus\": 1,\n  # ########################\n\n  # parallelism settings\n  \"pipe-parallel-size\": 1,\n  \"model-parallel-size\": 1,\n\n  # model settings\n  \"num-layers\": 12,\n  \"hidden-size\": 768,\n  \"num-attention-heads\": 12,\n  \"seq-length\": 2048,\n  \"max-position-embeddings\": 2048,\n  \"pos-emb\": \"rotary\",\n  \"rotary-pct\": 0.25,\n  \"no-weight-tying\": true,\n  \"gpt-j-residual\": true,\n  \"output-layer-parallelism\": \"column\",\n  \"attention-config\": [[[\"global\"], 12]], \n\n  \"scaled_masked_softmax_fusion\": true, # For SepLLM\n  \"bias-gelu-fusion\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.0006,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1.0e-8\n    }\n  },\n  \"min_lr\": 0.00006,\n\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": true,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": true,\n    \"reduce_scatter\": true,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": true,\n    \"cpu_offload\": false\n  },\n\n  # batch size (trained on 8 gpus)\n  \"train_micro_batch_size_per_gpu\": 64, # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"gradient_accumulation_steps\": 2,  # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"data-impl\": \"mmap\",\n  \"num_workers\": 1,\n\n  # activation checkpointing\n  \"checkpoint-activations\": true,\n  \"checkpoint-num-layers\": 1,\n  \"partition-activations\": true,\n  \"synchronize-each-layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight-decay\": 0.1,\n  \"hidden-dropout\": 0,\n  \"attention-dropout\": 0,\n\n  # precision settings\n  \"fp16\": {\n    \"fp16\": true,\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"initial_scale_power\": 12,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  \"train-iters\": 143000,\n  \"lr-decay-iters\": 143000,\n  \"distributed-backend\": \"nccl\",\n  \"lr-decay-style\": \"cosine\",\n  \"warmup\": 0.01,\n  \"checkpoint-factor\": 1000,\n  \"extra-save-iters\": [0,1,2,4,8,16,32,64,128,256,512],\n  \"eval-interval\": 4000, \n  \"eval-iters\": 10,\n\n  \"log-interval\": 10,\n  \"steps_per_print\": 10,\n  \"wall_clock_breakdown\": true,\n\n  \"train-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"valid-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"test-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n\n  \"tokenizer-type\": \"HFTokenizer\",\n  \"vocab-file\": \"/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json\",\n\n  \"launcher\": \"pdsh\",\n\n  \"save\": \"/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  \"load\": \"/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  \n  # \"save\": \"path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  # \"load\": \"path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n\n  ####################################################################### SepLLM #######################################################################:\n  # \"hostfile\": \"path/to/SepLLM/Training-SepLLM/sample_configs/hostfile\",\n\n  'separator_token_ids': [15, 13, 32, 2, 28, 27, 209, 186, 187], # For Pythia tokenizer  ## Fixed; The token ids for the special tokens (i.e. separators):  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'].\n  'PADDING_ID': 0 , # For Pythia tokenizer  ## Fixed; The id for padding token of Pythia (GPT_NeoX)\n  \n\n  'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS'  :   True,  # If True: the prefilling local window sizes for different self-attention layers are different; If True: should set 'prefill_loc_win_size_list', else: should set 'prefill_local_window_size'\n  'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS' :   True,  # If True: the generating local window sizes for different self-attention layers are different; If True: should set 'generate_win_loc_size_list', else: should set 'generate_local_window_size'. USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS does not have any effect during the pretraining/prefilling phase.    \n\n  'prefill_loc_win_size_list' : [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when training (or prefilling). KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token.                      \n\n  'generate_win_loc_size_list': [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when generating. KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token. generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.\n\n  'init_tok_max_idx' :  2, # The largest index for the kept initial tokens. E.g., if init_tok_max_idx==2, it means we keep 3 initial tokens (idx: 0,1,2)\n  ######################################There should be at most 1 True for the following 3 args ##############################################\n  'USE_ORIGINAL_FULL_ATTEN' : False,  # Flag signal with the highest priority.  Train the Pythia model without any modification (standard full-attention version, i.e., standard upper triangular mask) if True.\n  'streamingLLM' :  False,  # Train streamingLLM. Only takes effect when USE_ORIGINAL_FULL_ATTEN=False. \n  'USE_SEP_ATTN_KERNEL_ACCELERATOR': True, # If True, use Sep_Attention module's kernel accelerator to accelerate the training process of SepLLM. If False (together with USE_ORIGINAL_FULL_ATTEN=False and streamingLLM=False), run plain SepLLM\n  ######################################There should be at most 1 True for the above 3 args ##############################################\n\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n  'RECOMPILE_SEP_ATTN_KERNEL': True,  # False by default. If True, recompile the Sep_Attention kernels.  When set to True, it may require more GPU memory and provide a certain level of acceleration to the training process.\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n\n  'BATCH_ADAPTIVE_INIT_POS' : False,  # False by default.  If True: use the floating initial tokens' starting positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding). Can be False when pretraining since the starting positions of initial tokens are at the beginning of each sequence in a batch for pretraining (i.e., right padding)\n  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for the released trained model during generating). When pretraining, it will also print the retention ratio for the computational complexity of calculating the attention map if it is set True\n  'print_ratio_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_ratio_intervals' forward passes (or print_ratio_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    \n\n  'USE_BiPE':  True,  # False by default. If True (must also set pos_emb='rotary' or 'alibi'), use Bilevel Positional Encoding [He, Zhenyu, et al. \"Two stones hit one bird: Bilevel positional encoding for better length extrapolation.\" arXiv preprint arXiv:2401.16421 (2024).].\n  'BiPE_seps': [ 15, 13, 32, 2, 28, 27, 209, 186, 187 ],  # The token ids of the seperator tokens for BiPE. e.g.,  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'] \n  ####################################################################### ### #######################################################################\n}\n"}, "load": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE", "checkpoint_factor": 1000, "extra_save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512], "batch_size": 64, "train_iters": 143000, "eval_iters": 10, "eval_interval": 4000, "vocab_file": "/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json", "num_workers": 1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "log_interval": 10, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "../../train.py", "save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000], "global_num_gpus": 2, "separator_token_ids": [15, 13, 32, 2, 28, 27, 209, 186, 187], "USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS": true, "USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS": true, "prefill_loc_win_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], "generate_win_loc_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], "RECOMPILE_SEP_ATTN_KERNEL": true, "PRINT_KV_RATIO": true, "USE_BiPE": true, "BiPE_seps": [15, 13, 32, 2, 28, 27, 209, 186, 187]}
[2025-01-14 11:07:40,017] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /home/txiao/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2025-01-14 11:07:47,000] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-01-14 11:07:47,001] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-01-14 11:07:47,001] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-01-14 11:07:47,001] [INFO] [launch.py:164:main] dist_world_size=2
[2025-01-14 11:07:47,001] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-01-14 11:07:47,004] [INFO] [launch.py:256:main] process 112248 spawned with command: ['/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/bin/python', '-u', '../../train.py', '--local_rank=0', '--deepspeed_config', 'eyJ0cmFpbl9iYXRjaF9zaXplIjogMjU2LCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogNjQsICJncmFkaWVudF9hY2N1bXVsYXRpb25fc3RlcHMiOiAyLCAib3B0aW1pemVyIjogeyJ0eXBlIjogIkFkYW0iLCAicGFyYW1zIjogeyJsciI6IDAuMDAwNiwgImJldGFzIjogWzAuOSwgMC45NV0sICJlcHMiOiAxZS0wOH19LCAiZnAxNiI6IHsiZnAxNiI6IHRydWUsICJlbmFibGVkIjogdHJ1ZSwgImxvc3Nfc2NhbGUiOiAwLCAibG9zc19zY2FsZV93aW5kb3ciOiAxMDAwLCAiaW5pdGlhbF9zY2FsZV9wb3dlciI6IDEyLCAiaHlzdGVyZXNpcyI6IDIsICJtaW5fbG9zc19zY2FsZSI6IDF9LCAiemVyb19vcHRpbWl6YXRpb24iOiB7InN0YWdlIjogMSwgImFsbGdhdGhlcl9wYXJ0aXRpb25zIjogdHJ1ZSwgImFsbGdhdGhlcl9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgIm92ZXJsYXBfY29tbSI6IHRydWUsICJyZWR1Y2Vfc2NhdHRlciI6IHRydWUsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IHRydWUsICJjcHVfb2ZmbG9hZCI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZX0=', '--megatron_config', '{"train_batch_size": 256, "train_micro_batch_size_per_gpu": 64, "gradient_accumulation_steps": 2, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "initial_scale_power": 12, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 12, "hidden_size": 768, "num_attention_heads": 12, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "lr_decay_style": "cosine", "lr_decay_iters": 143000, "min_lr": 6e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0006, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "test_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "valid_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "train_data_weights": [1.0], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "save": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE", "config_files": {"sepllm-160m-on-pythia-with-pile_deduped-n64HT-kernel_recompile_rotaryBiPE.yml": "{\n  # ##########Debug##########\n  #   \"world_size\": 1,\n  #   \"num_gpus\": 1,\n  # ########################\n\n  # parallelism settings\n  \"pipe-parallel-size\": 1,\n  \"model-parallel-size\": 1,\n\n  # model settings\n  \"num-layers\": 12,\n  \"hidden-size\": 768,\n  \"num-attention-heads\": 12,\n  \"seq-length\": 2048,\n  \"max-position-embeddings\": 2048,\n  \"pos-emb\": \"rotary\",\n  \"rotary-pct\": 0.25,\n  \"no-weight-tying\": true,\n  \"gpt-j-residual\": true,\n  \"output-layer-parallelism\": \"column\",\n  \"attention-config\": [[[\"global\"], 12]], \n\n  \"scaled_masked_softmax_fusion\": true, # For SepLLM\n  \"bias-gelu-fusion\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.0006,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1.0e-8\n    }\n  },\n  \"min_lr\": 0.00006,\n\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": true,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": true,\n    \"reduce_scatter\": true,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": true,\n    \"cpu_offload\": false\n  },\n\n  # batch size (trained on 8 gpus)\n  \"train_micro_batch_size_per_gpu\": 64, # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"gradient_accumulation_steps\": 2,  # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"data-impl\": \"mmap\",\n  \"num_workers\": 1,\n\n  # activation checkpointing\n  \"checkpoint-activations\": true,\n  \"checkpoint-num-layers\": 1,\n  \"partition-activations\": true,\n  \"synchronize-each-layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight-decay\": 0.1,\n  \"hidden-dropout\": 0,\n  \"attention-dropout\": 0,\n\n  # precision settings\n  \"fp16\": {\n    \"fp16\": true,\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"initial_scale_power\": 12,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  \"train-iters\": 143000,\n  \"lr-decay-iters\": 143000,\n  \"distributed-backend\": \"nccl\",\n  \"lr-decay-style\": \"cosine\",\n  \"warmup\": 0.01,\n  \"checkpoint-factor\": 1000,\n  \"extra-save-iters\": [0,1,2,4,8,16,32,64,128,256,512],\n  \"eval-interval\": 4000, \n  \"eval-iters\": 10,\n\n  \"log-interval\": 10,\n  \"steps_per_print\": 10,\n  \"wall_clock_breakdown\": true,\n\n  \"train-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"valid-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"test-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n\n  \"tokenizer-type\": \"HFTokenizer\",\n  \"vocab-file\": \"/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json\",\n\n  \"launcher\": \"pdsh\",\n\n  \"save\": \"/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  \"load\": \"/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  \n  # \"save\": \"path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  # \"load\": \"path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n\n  ####################################################################### SepLLM #######################################################################:\n  # \"hostfile\": \"path/to/SepLLM/Training-SepLLM/sample_configs/hostfile\",\n\n  'separator_token_ids': [15, 13, 32, 2, 28, 27, 209, 186, 187], # For Pythia tokenizer  ## Fixed; The token ids for the special tokens (i.e. separators):  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'].\n  'PADDING_ID': 0 , # For Pythia tokenizer  ## Fixed; The id for padding token of Pythia (GPT_NeoX)\n  \n\n  'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS'  :   True,  # If True: the prefilling local window sizes for different self-attention layers are different; If True: should set 'prefill_loc_win_size_list', else: should set 'prefill_local_window_size'\n  'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS' :   True,  # If True: the generating local window sizes for different self-attention layers are different; If True: should set 'generate_win_loc_size_list', else: should set 'generate_local_window_size'. USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS does not have any effect during the pretraining/prefilling phase.    \n\n  'prefill_loc_win_size_list' : [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when training (or prefilling). KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token.                      \n\n  'generate_win_loc_size_list': [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when generating. KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token. generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.\n\n  'init_tok_max_idx' :  2, # The largest index for the kept initial tokens. E.g., if init_tok_max_idx==2, it means we keep 3 initial tokens (idx: 0,1,2)\n  ######################################There should be at most 1 True for the following 3 args ##############################################\n  'USE_ORIGINAL_FULL_ATTEN' : False,  # Flag signal with the highest priority.  Train the Pythia model without any modification (standard full-attention version, i.e., standard upper triangular mask) if True.\n  'streamingLLM' :  False,  # Train streamingLLM. Only takes effect when USE_ORIGINAL_FULL_ATTEN=False. \n  'USE_SEP_ATTN_KERNEL_ACCELERATOR': True, # If True, use Sep_Attention module's kernel accelerator to accelerate the training process of SepLLM. If False (together with USE_ORIGINAL_FULL_ATTEN=False and streamingLLM=False), run plain SepLLM\n  ######################################There should be at most 1 True for the above 3 args ##############################################\n\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n  'RECOMPILE_SEP_ATTN_KERNEL': True,  # False by default. If True, recompile the Sep_Attention kernels.  When set to True, it may require more GPU memory and provide a certain level of acceleration to the training process.\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n\n  'BATCH_ADAPTIVE_INIT_POS' : False,  # False by default.  If True: use the floating initial tokens' starting positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding). Can be False when pretraining since the starting positions of initial tokens are at the beginning of each sequence in a batch for pretraining (i.e., right padding)\n  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for the released trained model during generating). When pretraining, it will also print the retention ratio for the computational complexity of calculating the attention map if it is set True\n  'print_ratio_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_ratio_intervals' forward passes (or print_ratio_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    \n\n  'USE_BiPE':  True,  # False by default. If True (must also set pos_emb='rotary' or 'alibi'), use Bilevel Positional Encoding [He, Zhenyu, et al. \"Two stones hit one bird: Bilevel positional encoding for better length extrapolation.\" arXiv preprint arXiv:2401.16421 (2024).].\n  'BiPE_seps': [ 15, 13, 32, 2, 28, 27, 209, 186, 187 ],  # The token ids of the seperator tokens for BiPE. e.g.,  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'] \n  ####################################################################### ### #######################################################################\n}\n"}, "load": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE", "checkpoint_factor": 1000, "extra_save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512], "batch_size": 64, "train_iters": 143000, "eval_iters": 10, "eval_interval": 4000, "vocab_file": "/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json", "num_workers": 1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "log_interval": 10, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "../../train.py", "save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000], "global_num_gpus": 2, "separator_token_ids": [15, 13, 32, 2, 28, 27, 209, 186, 187], "USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS": true, "USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS": true, "prefill_loc_win_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], "generate_win_loc_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], "RECOMPILE_SEP_ATTN_KERNEL": true, "PRINT_KV_RATIO": true, "USE_BiPE": true, "BiPE_seps": [15, 13, 32, 2, 28, 27, 209, 186, 187]}']
[2025-01-14 11:07:47,007] [INFO] [launch.py:256:main] process 112249 spawned with command: ['/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/bin/python', '-u', '../../train.py', '--local_rank=1', '--deepspeed_config', 'eyJ0cmFpbl9iYXRjaF9zaXplIjogMjU2LCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogNjQsICJncmFkaWVudF9hY2N1bXVsYXRpb25fc3RlcHMiOiAyLCAib3B0aW1pemVyIjogeyJ0eXBlIjogIkFkYW0iLCAicGFyYW1zIjogeyJsciI6IDAuMDAwNiwgImJldGFzIjogWzAuOSwgMC45NV0sICJlcHMiOiAxZS0wOH19LCAiZnAxNiI6IHsiZnAxNiI6IHRydWUsICJlbmFibGVkIjogdHJ1ZSwgImxvc3Nfc2NhbGUiOiAwLCAibG9zc19zY2FsZV93aW5kb3ciOiAxMDAwLCAiaW5pdGlhbF9zY2FsZV9wb3dlciI6IDEyLCAiaHlzdGVyZXNpcyI6IDIsICJtaW5fbG9zc19zY2FsZSI6IDF9LCAiemVyb19vcHRpbWl6YXRpb24iOiB7InN0YWdlIjogMSwgImFsbGdhdGhlcl9wYXJ0aXRpb25zIjogdHJ1ZSwgImFsbGdhdGhlcl9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgIm92ZXJsYXBfY29tbSI6IHRydWUsICJyZWR1Y2Vfc2NhdHRlciI6IHRydWUsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IHRydWUsICJjcHVfb2ZmbG9hZCI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZX0=', '--megatron_config', '{"train_batch_size": 256, "train_micro_batch_size_per_gpu": 64, "gradient_accumulation_steps": 2, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "initial_scale_power": 12, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 12, "hidden_size": 768, "num_attention_heads": 12, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "lr_decay_style": "cosine", "lr_decay_iters": 143000, "min_lr": 6e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0006, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "test_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "valid_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "train_data_weights": [1.0], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "save": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE", "config_files": {"sepllm-160m-on-pythia-with-pile_deduped-n64HT-kernel_recompile_rotaryBiPE.yml": "{\n  # ##########Debug##########\n  #   \"world_size\": 1,\n  #   \"num_gpus\": 1,\n  # ########################\n\n  # parallelism settings\n  \"pipe-parallel-size\": 1,\n  \"model-parallel-size\": 1,\n\n  # model settings\n  \"num-layers\": 12,\n  \"hidden-size\": 768,\n  \"num-attention-heads\": 12,\n  \"seq-length\": 2048,\n  \"max-position-embeddings\": 2048,\n  \"pos-emb\": \"rotary\",\n  \"rotary-pct\": 0.25,\n  \"no-weight-tying\": true,\n  \"gpt-j-residual\": true,\n  \"output-layer-parallelism\": \"column\",\n  \"attention-config\": [[[\"global\"], 12]], \n\n  \"scaled_masked_softmax_fusion\": true, # For SepLLM\n  \"bias-gelu-fusion\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.0006,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1.0e-8\n    }\n  },\n  \"min_lr\": 0.00006,\n\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": true,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": true,\n    \"reduce_scatter\": true,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": true,\n    \"cpu_offload\": false\n  },\n\n  # batch size (trained on 8 gpus)\n  \"train_micro_batch_size_per_gpu\": 64, # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"gradient_accumulation_steps\": 2,  # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"data-impl\": \"mmap\",\n  \"num_workers\": 1,\n\n  # activation checkpointing\n  \"checkpoint-activations\": true,\n  \"checkpoint-num-layers\": 1,\n  \"partition-activations\": true,\n  \"synchronize-each-layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight-decay\": 0.1,\n  \"hidden-dropout\": 0,\n  \"attention-dropout\": 0,\n\n  # precision settings\n  \"fp16\": {\n    \"fp16\": true,\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"initial_scale_power\": 12,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  \"train-iters\": 143000,\n  \"lr-decay-iters\": 143000,\n  \"distributed-backend\": \"nccl\",\n  \"lr-decay-style\": \"cosine\",\n  \"warmup\": 0.01,\n  \"checkpoint-factor\": 1000,\n  \"extra-save-iters\": [0,1,2,4,8,16,32,64,128,256,512],\n  \"eval-interval\": 4000, \n  \"eval-iters\": 10,\n\n  \"log-interval\": 10,\n  \"steps_per_print\": 10,\n  \"wall_clock_breakdown\": true,\n\n  \"train-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"valid-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"test-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n\n  \"tokenizer-type\": \"HFTokenizer\",\n  \"vocab-file\": \"/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json\",\n\n  \"launcher\": \"pdsh\",\n\n  \"save\": \"/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  \"load\": \"/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  \n  # \"save\": \"path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n  # \"load\": \"path/to/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE\",\n\n  ####################################################################### SepLLM #######################################################################:\n  # \"hostfile\": \"path/to/SepLLM/Training-SepLLM/sample_configs/hostfile\",\n\n  'separator_token_ids': [15, 13, 32, 2, 28, 27, 209, 186, 187], # For Pythia tokenizer  ## Fixed; The token ids for the special tokens (i.e. separators):  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'].\n  'PADDING_ID': 0 , # For Pythia tokenizer  ## Fixed; The id for padding token of Pythia (GPT_NeoX)\n  \n\n  'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS'  :   True,  # If True: the prefilling local window sizes for different self-attention layers are different; If True: should set 'prefill_loc_win_size_list', else: should set 'prefill_local_window_size'\n  'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS' :   True,  # If True: the generating local window sizes for different self-attention layers are different; If True: should set 'generate_win_loc_size_list', else: should set 'generate_local_window_size'. USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS does not have any effect during the pretraining/prefilling phase.    \n\n  'prefill_loc_win_size_list' : [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when training (or prefilling). KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token.                      \n\n  'generate_win_loc_size_list': [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   2048],            ## The local window sizes for different self-attention layers when generating. KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token. generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.\n\n  'init_tok_max_idx' :  2, # The largest index for the kept initial tokens. E.g., if init_tok_max_idx==2, it means we keep 3 initial tokens (idx: 0,1,2)\n  ######################################There should be at most 1 True for the following 3 args ##############################################\n  'USE_ORIGINAL_FULL_ATTEN' : False,  # Flag signal with the highest priority.  Train the Pythia model without any modification (standard full-attention version, i.e., standard upper triangular mask) if True.\n  'streamingLLM' :  False,  # Train streamingLLM. Only takes effect when USE_ORIGINAL_FULL_ATTEN=False. \n  'USE_SEP_ATTN_KERNEL_ACCELERATOR': True, # If True, use Sep_Attention module's kernel accelerator to accelerate the training process of SepLLM. If False (together with USE_ORIGINAL_FULL_ATTEN=False and streamingLLM=False), run plain SepLLM\n  ######################################There should be at most 1 True for the above 3 args ##############################################\n\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n  'RECOMPILE_SEP_ATTN_KERNEL': True,  # False by default. If True, recompile the Sep_Attention kernels.  When set to True, it may require more GPU memory and provide a certain level of acceleration to the training process.\n  ###############You can set RECOMPILE_SEP_ATTN_KERNEL=True if you have enough GPU memory#################\n\n  'BATCH_ADAPTIVE_INIT_POS' : False,  # False by default.  If True: use the floating initial tokens' starting positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding). Can be False when pretraining since the starting positions of initial tokens are at the beginning of each sequence in a batch for pretraining (i.e., right padding)\n  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for the released trained model during generating). When pretraining, it will also print the retention ratio for the computational complexity of calculating the attention map if it is set True\n  'print_ratio_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_ratio_intervals' forward passes (or print_ratio_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    \n\n  'USE_BiPE':  True,  # False by default. If True (must also set pos_emb='rotary' or 'alibi'), use Bilevel Positional Encoding [He, Zhenyu, et al. \"Two stones hit one bird: Bilevel positional encoding for better length extrapolation.\" arXiv preprint arXiv:2401.16421 (2024).].\n  'BiPE_seps': [ 15, 13, 32, 2, 28, 27, 209, 186, 187 ],  # The token ids of the seperator tokens for BiPE. e.g.,  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'] \n  ####################################################################### ### #######################################################################\n}\n"}, "load": "/lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE", "checkpoint_factor": 1000, "extra_save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512], "batch_size": 64, "train_iters": 143000, "eval_iters": 10, "eval_interval": 4000, "vocab_file": "/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json", "num_workers": 1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "log_interval": 10, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "../../train.py", "save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000], "global_num_gpus": 2, "separator_token_ids": [15, 13, 32, 2, 28, 27, 209, 186, 187], "USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS": true, "USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS": true, "prefill_loc_win_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], "generate_win_loc_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048], "RECOMPILE_SEP_ATTN_KERNEL": true, "PRINT_KV_RATIO": true, "USE_BiPE": true, "BiPE_seps": [15, 13, 32, 2, 28, 27, 209, 186, 187]}']
[2025-01-14 11:07:51,571] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /home/txiao/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2025-01-14 11:07:56,652] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /home/txiao/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  True
self.RECOMPILE_SEP_ATTN_KERNEL:  True
self.BATCH_ADAPTIVE_INIT_POS:  False
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the separator_token_ids, Make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: True
self.BiPE_seps: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[2025-01-14 11:08:03,972] [INFO] [comm.py:637:init_distributed] cdb=None
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
NeoXArgs.configure_distributed_args() using world size: 2 and model-parallel size: 1 
> building HFTokenizer tokenizer ...
 > padded vocab (size: 50277) with 27 dummy tokens (new size: 50304)
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 2048]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  True
self.RECOMPILE_SEP_ATTN_KERNEL:  True
self.BATCH_ADAPTIVE_INIT_POS:  False
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the separator_token_ids, Make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: True
self.BiPE_seps: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: guoxchen (guoxchen-hku). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/training_examples/example160m_n64HT_kernel_recompile_rotaryBiPE/wandb/run-20250114_110805-pwjf9nv2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sea-498
wandb: ⭐️ View project at https://wandb.ai/guoxchen-hku/neox
wandb: 🚀 View run at https://wandb.ai/guoxchen-hku/neox/runs/pwjf9nv2
Detected CUDA files, patching ldflags
Emitting ninja build file /lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/fused_kernels/build/build.ninja...
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/fused_kernels/build/build.ninja...
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/fused_kernels/build/build.ninja...
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_rotary_positional_embedding...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_rotary_positional_embedding...
> initializing torch distributed ...
[2025-01-14 11:08:08,247] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-14 11:08:08,248] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> initializing model parallel with size 1
MPU DP: [0, 1]
MPU PP: [0]
MPU PP: [1]
MPU MP: [0]
MPU MP: [1]
> setting random seeds to 1234 ...
[2025-01-14 11:08:08,331] [INFO] [checkpointing.py:229:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/data'
WARNING: APEX not installed - defaulting to deepspeed's fused adam
[2025-01-14 11:08:08,507] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
make: Nothing to be done for 'default'.
make: Leaving directory '/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/data'
building GPT2 model ...
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1}
[2025-01-14 11:08:08,984] [INFO] [module.py:392:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp
stage=0 layers=17
     0: EmbeddingPipe
     1: <megatron.model.gpt2_model._pre_transformer_block object at 0x14f02fd656c0>
     2: ParallelTransformerLayerPipe
     3: ParallelTransformerLayerPipe
     4: ParallelTransformerLayerPipe
     5: ParallelTransformerLayerPipe
     6: ParallelTransformerLayerPipe
     7: ParallelTransformerLayerPipe
     8: ParallelTransformerLayerPipe
     9: ParallelTransformerLayerPipe
    10: ParallelTransformerLayerPipe
    11: ParallelTransformerLayerPipe
    12: ParallelTransformerLayerPipe
    13: ParallelTransformerLayerPipe
    14: <megatron.model.gpt2_model._post_transformer_block object at 0x14f02fd652d0>
    15: NormPipe
    16: ParallelLinearPipe
  loss: partial
Configuring Optimizer type: Adam with params: {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}
WARNING: APEX not installed - defaulting to deepspeed's fused adam
> learning rate decay style: cosine
DeepSpeed is enabled.
[2025-01-14 11:08:09,253] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.5+unknown, git-hash=unknown, git-branch=unknown
[2025-01-14 11:08:09,254] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[2025-01-14 11:08:10,655] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-14 11:08:10,657] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-01-14 11:08:10,658] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-14 11:08:10,660] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-14 11:08:10,660] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-01-14 11:08:10,661] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2025-01-14 11:08:10,661] [WARNING] [engine.py:1549:_configure_zero_optimizer] Pipeline parallelism does not support overlapped communication, will be disabled.
[2025-01-14 11:08:10,661] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500000000
[2025-01-14 11:08:10,661] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500000000
[2025-01-14 11:08:10,661] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2025-01-14 11:08:10,661] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2025-01-14 11:08:10,665] [WARNING] [engine.py:1549:_configure_zero_optimizer] Pipeline parallelism does not support overlapped communication, will be disabled.
[2025-01-14 11:08:11,888] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-14 11:08:12,029] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-01-14 11:08:12,030] [INFO] [utils.py:782:see_memory_usage] MA 0.61 GB         Max_MA 0.61 GB         CA 0.62 GB         Max_CA 1 GB 
[2025-01-14 11:08:12,033] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 3.04 GB, percent = 1.0%
[2025-01-14 11:08:12,227] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-01-14 11:08:12,228] [INFO] [utils.py:782:see_memory_usage] MA 0.61 GB         Max_MA 0.92 GB         CA 0.92 GB         Max_CA 1 GB 
[2025-01-14 11:08:12,231] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 3.04 GB, percent = 1.0%
[2025-01-14 11:08:12,232] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2025-01-14 11:08:12,419] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-01-14 11:08:12,420] [INFO] [utils.py:782:see_memory_usage] MA 0.61 GB         Max_MA 0.61 GB         CA 0.92 GB         Max_CA 1 GB 
[2025-01-14 11:08:12,424] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 3.04 GB, percent = 1.0%
[2025-01-14 11:08:12,426] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-01-14 11:08:12,426] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-14 11:08:12,426] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x14f02fd12290>
[2025-01-14 11:08:12,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-14 11:08:12,427] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2025-01-14 11:08:12,427] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-14 11:08:12,427] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-14 11:08:12,427] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2025-01-14 11:08:12,427] [INFO] [config.py:1001:print]   amp_params ................... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   bfloat16_enabled ............. False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14f02fd11f90>
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   dump_state ................... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-14 11:08:12,428] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   fp16_auto_cast ............... False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   fp16_enabled ................. True
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   global_rank .................. 0
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 2
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   gradient_clipping ............ 0.0
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 4096
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   loss_scale ................... 0
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2025-01-14 11:08:12,429] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   optimizer_name ............... adam
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   optimizer_params ............. {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   pld_params ................... False
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   steps_per_print .............. 10
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   train_batch_size ............. 256
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  64
[2025-01-14 11:08:12,430] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... True
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   world_size ................... 2
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  False
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-14 11:08:12,431] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 1
[2025-01-14 11:08:12,431] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 64, 
    "gradient_accumulation_steps": 2, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0006, 
            "betas": [0.9, 0.95], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "fp16": true, 
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 12, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 1, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "cpu_offload": false
    }, 
    "wall_clock_breakdown": true
}
[2025-01-14 11:08:12,431] [INFO] [engine.py:100:__init__] CONFIG: micro_batches=2 micro_batch_size=64
[2025-01-14 11:08:12,431] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-14 11:08:12,596] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=17 [0, 17) STAGE_PARAMS=163895808 (163.896M) TOTAL_PARAMS=163895808 (163.896M) UNIQUE_PARAMS=163895808 (163.896M)
 > number of parameters on model parallel rank 0: 163895808
 > total params: 163,895,808
[2025-01-14 11:08:13,362] [WARNING] [engine.py:2796:load_checkpoint] Unable to find latest file at /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-14 11:08:13,363] [WARNING] [engine.py:2796:load_checkpoint] Unable to find latest file at /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Unable to load checkpoint.
Loading checkpoint and starting from iteration 0
> building train, validation, and test datasets ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
    train_0:
     no. of documents:134318121
/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/data/gpt2_dataset.py:338: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/megatron/data/gpt2_dataset.py:338: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
 > loading doc-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_train_0_indexmap_36791040ns_2048sl_1234s_packedpi_ac_doc_idx.npy
 > loading sample-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_train_0_indexmap_36791040ns_2048sl_1234s_packedpi_ac_sample_idx.npy
 > loading shuffle-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_train_0_indexmap_36791040ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
    loaded indexed file in 0.075 seconds
    total number of samples: 101157429
    total number of epochs: 1
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
    valid_0:
     no. of documents:134318121
 > loading doc-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_valid_0_indexmap_92621ns_2048sl_1234s_packedpi_ac_doc_idx.npy
 > loading sample-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_valid_0_indexmap_92621ns_2048sl_1234s_packedpi_ac_sample_idx.npy
 > loading shuffle-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_valid_0_indexmap_92621ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
    loaded indexed file in 0.070 seconds
    total number of samples: 101157429
    total number of epochs: 1
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
    test_0:
     no. of documents:134318121
 > loading doc-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_test_0_indexmap_2573ns_2048sl_1234s_packedpi_ac_doc_idx.npy
 > loading sample-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_test_0_indexmap_2573ns_2048sl_1234s_packedpi_ac_sample_idx.npy
 > loading shuffle-idx mapping from /lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document_test_0_indexmap_2573ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
    loaded indexed file in 0.058 seconds
    total number of samples: 101157429
    total number of epochs: 1
> building indices for blendable datasets ...
 > sample ratios:
   dataset 0, input: 1, achieved: 1
> RANK 0 elapsed time for building blendable dataset indices: 0.78 (sec)
> building indices for blendable datasets ...
> RANK 1 elapsed time for building blendable dataset indices: 0.78 (sec)
> RANK 1 elapsed time for building blendable dataset indices: 0.75 (sec)
 > sample ratios:
   dataset 0, input: 1, achieved: 1
> RANK 0 elapsed time for building blendable dataset indices: 0.76 (sec)
> building indices for blendable datasets ...
> RANK 1 elapsed time for building blendable dataset indices: 0.74 (sec)
 > sample ratios:
   dataset 0, input: 1, achieved: 1
> RANK 0 elapsed time for building blendable dataset indices: 0.77 (sec)
setting training data start iteration to 0
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 4385.41 | train/valid/test data iterators: 11131.05
training ...
[2025-01-14 11:08:24,508] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step0 is about to be saved!
[2025-01-14 11:08:24,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_00-model_00-model_states.pt...
[2025-01-14 11:08:24,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_00-model_00-model_states.pt.
[2025-01-14 11:08:24,805] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_02-model_00-model_states.pt...
[2025-01-14 11:08:24,831] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_02-model_00-model_states.pt.
[2025-01-14 11:08:24,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_03-model_00-model_states.pt...
[2025-01-14 11:08:24,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_03-model_00-model_states.pt.
[2025-01-14 11:08:24,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_04-model_00-model_states.pt...
[2025-01-14 11:08:24,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_04-model_00-model_states.pt.
[2025-01-14 11:08:24,922] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_05-model_00-model_states.pt...
[2025-01-14 11:08:24,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_05-model_00-model_states.pt.
[2025-01-14 11:08:24,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_06-model_00-model_states.pt...
[2025-01-14 11:08:24,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_06-model_00-model_states.pt.
[2025-01-14 11:08:24,981] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_07-model_00-model_states.pt...
[2025-01-14 11:08:25,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_07-model_00-model_states.pt.
[2025-01-14 11:08:25,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_08-model_00-model_states.pt...
[2025-01-14 11:08:25,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_08-model_00-model_states.pt.
[2025-01-14 11:08:25,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_09-model_00-model_states.pt...
[2025-01-14 11:08:25,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_09-model_00-model_states.pt.
[2025-01-14 11:08:25,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_10-model_00-model_states.pt...
[2025-01-14 11:08:25,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_10-model_00-model_states.pt.
[2025-01-14 11:08:25,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_11-model_00-model_states.pt...
[2025-01-14 11:08:25,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_11-model_00-model_states.pt.
[2025-01-14 11:08:25,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_12-model_00-model_states.pt...
[2025-01-14 11:08:25,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_12-model_00-model_states.pt.
[2025-01-14 11:08:25,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_13-model_00-model_states.pt...
[2025-01-14 11:08:25,216] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_13-model_00-model_states.pt.
[2025-01-14 11:08:25,217] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_15-model_00-model_states.pt...
[2025-01-14 11:08:25,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_15-model_00-model_states.pt.
[2025-01-14 11:08:25,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_16-model_00-model_states.pt...
[2025-01-14 11:08:25,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/layer_16-model_00-model_states.pt.
[2025-01-14 11:08:25,593] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/mp_rank_00_model_states.pt
[2025-01-14 11:08:25,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/mp_rank_00_model_states.pt...
[2025-01-14 11:08:25,600] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/mp_rank_00_model_states.pt.
[2025-01-14 11:08:25,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-01-14 11:08:25,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-01-14 11:08:26,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-01-14 11:08:27,003] [INFO] [engine.py:3527:_save_zero_checkpoint] zero checkpoint saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-01-14 11:08:27,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step0 is ready now!
[2025-01-14 11:08:27,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-01-14 11:08:27,067] [INFO] [engine.py:3527:_save_zero_checkpoint] zero checkpoint saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step0/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-01-14 11:08:27,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step0 is ready now!
[2025-01-14 11:08:33,973] [INFO] [checkpointing.py:541:forward] Activation Checkpointing Information
[2025-01-14 11:08:33,973] [INFO] [checkpointing.py:542:forward] ----Partition Activations True, CPU CHECKPOINTING False
[2025-01-14 11:08:33,973] [INFO] [checkpointing.py:543:forward] ----contiguous Memory Checkpointing False with 12 total layers
[2025-01-14 11:08:33,974] [INFO] [checkpointing.py:545:forward] ----Synchronization True
[2025-01-14 11:08:33,974] [INFO] [checkpointing.py:546:forward] ----Profiling time in checkpointing False
[2025-01-14 11:08:40,313] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1.89 | optimizer_gradients: 0.92 | optimizer_step: 1.82
[2025-01-14 11:08:41,836] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1 is about to be saved!
[2025-01-14 11:08:41,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_00-model_00-model_states.pt...
[2025-01-14 11:08:42,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_00-model_00-model_states.pt.
[2025-01-14 11:08:42,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_02-model_00-model_states.pt...
[2025-01-14 11:08:42,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_02-model_00-model_states.pt.
[2025-01-14 11:08:42,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_03-model_00-model_states.pt...
[2025-01-14 11:08:42,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_03-model_00-model_states.pt.
[2025-01-14 11:08:42,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_04-model_00-model_states.pt...
[2025-01-14 11:08:42,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_04-model_00-model_states.pt.
[2025-01-14 11:08:42,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_05-model_00-model_states.pt...
[2025-01-14 11:08:42,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_05-model_00-model_states.pt.
[2025-01-14 11:08:42,193] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_06-model_00-model_states.pt...
[2025-01-14 11:08:42,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_06-model_00-model_states.pt.
[2025-01-14 11:08:42,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_07-model_00-model_states.pt...
[2025-01-14 11:08:42,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_07-model_00-model_states.pt.
[2025-01-14 11:08:42,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_08-model_00-model_states.pt...
[2025-01-14 11:08:42,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_08-model_00-model_states.pt.
[2025-01-14 11:08:42,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_09-model_00-model_states.pt...
[2025-01-14 11:08:42,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_09-model_00-model_states.pt.
[2025-01-14 11:08:42,314] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_10-model_00-model_states.pt...
[2025-01-14 11:08:42,340] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_10-model_00-model_states.pt.
[2025-01-14 11:08:42,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_11-model_00-model_states.pt...
[2025-01-14 11:08:42,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_11-model_00-model_states.pt.
[2025-01-14 11:08:42,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_12-model_00-model_states.pt...
[2025-01-14 11:08:42,404] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_12-model_00-model_states.pt.
[2025-01-14 11:08:42,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_13-model_00-model_states.pt...
[2025-01-14 11:08:42,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_13-model_00-model_states.pt.
[2025-01-14 11:08:42,434] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_15-model_00-model_states.pt...
[2025-01-14 11:08:42,440] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_15-model_00-model_states.pt.
[2025-01-14 11:08:42,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_16-model_00-model_states.pt...
[2025-01-14 11:08:42,840] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/layer_16-model_00-model_states.pt.
[2025-01-14 11:08:42,845] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/mp_rank_00_model_states.pt
[2025-01-14 11:08:42,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/mp_rank_00_model_states.pt...
[2025-01-14 11:08:42,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/mp_rank_00_model_states.pt.
[2025-01-14 11:08:42,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-01-14 11:08:42,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-01-14 11:08:46,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-01-14 11:08:46,874] [INFO] [engine.py:3527:_save_zero_checkpoint] zero checkpoint saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-01-14 11:08:46,874] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2025-01-14 11:08:47,020] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-01-14 11:08:47,035] [INFO] [engine.py:3527:_save_zero_checkpoint] zero checkpoint saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step1/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-01-14 11:08:47,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1 is ready now!
[2025-01-14 11:08:48,824] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1.77 | optimizer_gradients: 0.88 | optimizer_step: 1.49
[2025-01-14 11:08:48,829] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2 is about to be saved!
[2025-01-14 11:08:48,919] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_00-model_00-model_states.pt...
[2025-01-14 11:08:49,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_00-model_00-model_states.pt.
[2025-01-14 11:08:49,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_02-model_00-model_states.pt...
[2025-01-14 11:08:49,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_02-model_00-model_states.pt.
[2025-01-14 11:08:49,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_03-model_00-model_states.pt...
[2025-01-14 11:08:49,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_03-model_00-model_states.pt.
[2025-01-14 11:08:49,135] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_04-model_00-model_states.pt...
[2025-01-14 11:08:49,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_04-model_00-model_states.pt.
[2025-01-14 11:08:49,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_05-model_00-model_states.pt...
[2025-01-14 11:08:49,195] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_05-model_00-model_states.pt.
[2025-01-14 11:08:49,200] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_06-model_00-model_states.pt...
[2025-01-14 11:08:49,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_06-model_00-model_states.pt.
[2025-01-14 11:08:49,230] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_07-model_00-model_states.pt...
[2025-01-14 11:08:49,256] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_07-model_00-model_states.pt.
[2025-01-14 11:08:49,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_08-model_00-model_states.pt...
[2025-01-14 11:08:49,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_08-model_00-model_states.pt.
[2025-01-14 11:08:49,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_09-model_00-model_states.pt...
[2025-01-14 11:08:49,319] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_09-model_00-model_states.pt.
[2025-01-14 11:08:49,323] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_10-model_00-model_states.pt...
[2025-01-14 11:08:49,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_10-model_00-model_states.pt.
[2025-01-14 11:08:49,353] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_11-model_00-model_states.pt...
[2025-01-14 11:08:49,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_11-model_00-model_states.pt.
[2025-01-14 11:08:49,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_12-model_00-model_states.pt...
[2025-01-14 11:08:49,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_12-model_00-model_states.pt.
[2025-01-14 11:08:49,411] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_13-model_00-model_states.pt...
[2025-01-14 11:08:49,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_13-model_00-model_states.pt.
[2025-01-14 11:08:49,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_15-model_00-model_states.pt...
[2025-01-14 11:08:49,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_15-model_00-model_states.pt.
[2025-01-14 11:08:49,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_16-model_00-model_states.pt...
[2025-01-14 11:08:49,721] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/layer_16-model_00-model_states.pt.
[2025-01-14 11:08:49,726] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/mp_rank_00_model_states.pt
[2025-01-14 11:08:49,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/mp_rank_00_model_states.pt...
[2025-01-14 11:08:49,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/mp_rank_00_model_states.pt.
[2025-01-14 11:08:49,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-01-14 11:08:49,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-01-14 11:08:52,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-01-14 11:08:52,419] [INFO] [engine.py:3527:_save_zero_checkpoint] zero checkpoint saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-01-14 11:08:52,419] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2 is ready now!
[2025-01-14 11:08:52,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-01-14 11:08:52,482] [INFO] [engine.py:3527:_save_zero_checkpoint] zero checkpoint saved /lustre/fast/fast/txiao/shihan/saves/SepLLM-160m/checkpoints_n64ht_8cards_kernel_recompile_rotaryBiPE/global_step2/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-01-14 11:08:52,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2 is ready now!
