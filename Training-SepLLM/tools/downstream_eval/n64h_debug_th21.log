2025-01-17:09:34:58,163 INFO     [utils.py:148] Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2025-01-17:09:34:58,854 INFO     [config.py:59] PyTorch version 2.1.0+cu121 available.
2025-01-17:09:35:14,441 INFO     [__main__.py:132] Verbosity set to INFO
2025-01-17:09:35:28,416 INFO     [__main__.py:205] Selected Tasks: ['arc_challenge', 'arc_easy', 'lambada_openai', 'logiqa', 'piqa', 'sciq', 'wikitext', 'winogrande', 'wsc']
2025-01-17:09:35:28,422 WARNING  [evaluator.py:93] generation_kwargs specified through cli, these settings will be used over set parameters in yaml tasks.
2025-01-17:09:35:29,174 INFO     [huggingface.py:120] Using device 'cuda:0'
Traceback (most recent call last):
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1128, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 825, in __getitem__
    raise KeyError(key)
KeyError: 'sepllm_gpt_neox'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/lm_eval/__main__.py", line 207, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/lm_eval/utils.py", line 402, in _wrapper
    return fn(*args, **kwargs)
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/lm_eval/evaluator.py", line 102, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/lm_eval/api/model.py", line 136, in create_from_arg_string
    return cls(**args, **args2)
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/lm_eval/models/huggingface.py", line 155, in __init__
    self._config = transformers.AutoConfig.from_pretrained(
  File "/home/txiao/miniconda3/envs/py38_cu121_torch21_new/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py", line 1130, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `sepllm_gpt_neox` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
